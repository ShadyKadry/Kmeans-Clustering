var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Geting-Started-with-KMeansClustering.jl","page":"Getting Started","title":"Geting Started with KMeansClustering.jl","text":"This is a short tutorial on how to use the KMeansClustering Library and visualize the results.","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"There are two options:\n\nInstallation using a local directory\n\n    pkg> add <path to library/Kmeans-Clustering>\n\nInstallation using git\n\n    pkg> add https://github.com/ShadyKadry/Kmeans-Clustering","category":"section"},{"location":"getting_started/#Basic-API","page":"Getting Started","title":"Basic API","text":"Importing is as easy as with any other package:\n\n    using KMeansClustering\n\n    # Used to select a predictable Random Numbers generator\n    using Random\n\n    # For Plotting\n    using Plots\n\nOnly one function is exposed:\n\n    my_rng = MersenneTwister(1234) # Number Generator with fixed seed\n\n    X = rand(my_rng, 2, 200) # Create an artificial dataset\n    cluster_count = 4 # Number of clusters to separate the dataset into\n\n    clustering_result = KMeansClustering.kmeans(\n        # Points, column-wise: rows are the features, cols are the points\n        X,\n\n        cluster_count,\n        # Select the KMeans-method to use\n        method=:kmeans,\n        # Select, how the initial centroids should be chosen\n        init=:random,\n        # Maximum number of iterations before the algorithm is aborted\n        maxiter=50,\n        # Tolerance of improvement between each iteration\n        tol=1e-4,\n        # Random Number Generator to use\n        rng=my_rng\n    )\n\n    @info \"Required Iterations: $(clustering_result.iterations)\"\n    @info \"Converged: $(clustering_result.converged)\"\n\nThe return value contains the result:\n\nclustering_result.centers: Matrix of center points\nclustering_result.assignments: Vector, that maps each original point to one of the centers in clustering_result.centers\nclustering_result.iterations: Number of iterations that was required\nclustering_result.converged: If true, the algorithm finished successfully (tol was reached). If false, maxiter was reached and the algorithm aborted","category":"section"},{"location":"getting_started/#Plotting","page":"Getting Started","title":"Plotting","text":"The result of the kmeans() function can be directly plotted using Plots.jl:\n\nscatter(\n    X[1, :], \n    X[2, :], \n    group=clustering_result.assignments, \n    legend=false\n)\n\nThe centers can be additionally marked:\n\nscatter!(\n    clustering_result.centers[1, :], \n    clustering_result.centers[2, :],\n    markersize=8,\n    marker=:star,\n    color=:black\n)","category":"section"},{"location":"getting_started/#Full-Script","page":"Getting Started","title":"Full Script","text":"    using KMeansClustering\n    using Random\n    using Plots\n\n    my_rng = MersenneTwister(1234) # Number Generator with fixed seed\n\n    X = rand(my_rng, 2, 200) # Create an artificial dataset\n    cluster_count = 4 # Number of clusters to separate the dataset into\n\n    clustering_result = KMeansClustering.kmeans(\n        X,                  # Points, column-wise: rows are the features, cols are the points\n        cluster_count,\n        method=:kmedoids,   # Select the KMeans-method to use\n        init=:random,\n        maxiter=50,\n        tol=1e-4,           # Tolerance of improvement between each iteration.\n        rng=my_rng          # Random Number Generator to use\n    )\n\n    @info \"Required Iterations: $(clustering_result.iterations)\"\n    @info \"Converged: $(clustering_result.converged)\"\n\n    scatter(\n        X[1, :], \n        X[2, :], \n        group=clustering_result.assignments, \n        legend=false\n    )\n\n    scatter!(\n        clustering_result.centers[1, :], \n        clustering_result.centers[2, :],\n        markersize=8,\n        marker=:star,\n        color=:black\n    )","category":"section"},{"location":"internal/#KMeansClustering.KMedoids.KMedoids_fit","page":"-","title":"KMeansClustering.KMedoids.KMedoids_fit","text":"KMedoids_fit(data, n_clusters; init_method=:random, max_iter=100,\n             tol=1e-4, rng=Random.GLOBAL_RNG, distance_fun=(a,b)->sum((a .- b).^2))\n\nPerform K-Medoids clustering on a dataset.\n\nK-Medoids is a clustering algorithm similar to k-means, but cluster centers (medoids) are always chosen from actual data points, making the algorithm more robust to noise and outliers.\n\nImplementation is based on the description from: http://leicestermath.org.uk/KmeansKmedoids/Kmeans_Kmedoids.html\n\nArguments\n\ndata::AbstractMatrix     A matrix of size (n_features, n_samples) where columns are data points   and rows are features.\nn_clusters::Integer     Number of clusters (i.e. number of medoids to compute).\n\nKeyword Arguments\n\ninit_method::Symbol = :random     Method for choosing initial medoids. Currently supported: :random.\nmax_iter::Integer = 100     Maximum number of refinement iterations.\ntol::Real = 1e-4     Minimum improvement required for convergence.\nrng::AbstractRNG = Random.GLOBAL_RNG     Random number generator.\ndistance_fun::Function     A function dist(a, b) returning the distance between two sample vectors.   Default is squared Euclidean distance.\n\nReturns a KMeansResult\n\n\n\n\n\n","category":"function"},{"location":"internal/#KMeansClustering.AlgorithmsKMeansPP.kmeanspp_init","page":"-","title":"KMeansClustering.AlgorithmsKMeansPP.kmeanspp_init","text":"kmeanspp_init(X, k; rng=Random.GLOBAL_RNG)\n\nSelect k initial centers using the k-means++ heuristic.\n\nArguments\n\nX: data matrix with features in rows and observations in columns.\nk: number of clusters.\n\nKeyword arguments\n\nrng: random number generator.\n\nReturns A vector of length k with indices into the columns of X, indicating which points are chosen as initial centers.\n\n\n\n\n\n","category":"function"},{"location":"internal/#KMeansClustering.KMedoids.KMedoids_Settings","page":"-","title":"KMeansClustering.KMedoids.KMedoids_Settings","text":"KMedoids_Settings\n\nSettings specific to the KMedoids algorithm\n\nFields:\n- `n_clusters`: Number of clusters that the dataset should be split up into\n- `max_iter`: Maximum number of iterations to run before aborting\n- `tol`: Tolerance for abortion. If the improvement between iterations is smaller than `tol`, the algorithm aborts\n- `rng`: Random Number Generator to use for generating the initial medoid centers\n- `distance_fun`: Cost function to calculate the distance between two points. This function must take two pairs of coordinates and return a number\n\n\n\n\n\n","category":"type"},{"location":"#KMeansClustering","page":"Home","title":"KMeansClustering","text":"Documentation for KMeansClustering.\n\nPages   = [\"index.md\"]","category":"section"},{"location":"#KMeansClustering.kmeans-Tuple{AbstractMatrix{<:Real}, Integer}","page":"Home","title":"KMeansClustering.kmeans","text":"kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=Random.GLOBAL_RNG)\n\nHigh-level entry point for k-means clustering.\n\nArguments\n\nX: data matrix with features in rows and observations in columns.\nk: number of clusters.\n\nKeyword arguments\n\nmethod: algorithm selector, see below (:kmedoids)\ninit: initialization strategy (:random, :kmeanspp).\nmaxiter: maximum number of Lloyd iterations.\ntol: tolerance for convergence.\nrng: random number generator.\n\nReturns a KMeansResult.\n\nAvailable algorithms:\n\nK-Medoids (method=:kmedoids):   As described by E.M. Mirkes, K-means and K-medoids applet. University of Leicester, 2011   Unlike typical K-Means, K-Medoids chooses its cluster centers from the given points X instead of calculating    artificial ones.\n\n\n\n\n\n","category":"method"},{"location":"#KMeansClustering.KMeansResult","page":"Home","title":"KMeansClustering.KMeansResult","text":"KMeansResult\n\nThis type stores the outcome of a k-means clustering run.\n\nConventions:\n\nThe data matrix X is assumed to have observations in columns and features in rows:   size(X, 1) = number of features   size(X, 2) = number of points\nThe centers matrix follows the same convention: each column is a cluster center.\n\nFields:\n\ncenters::Matrix{T}   The final cluster centers as a dÃ—k matrix, where d is the number of features   and k is the number of clusters.\nassignments::Vector{Int}   Cluster assignment for each data point as a vector of length n, where   n is the number of points (columns of X). The i-th entry is an integer   in 1:k indicating the cluster index of point i.\ninertia::T   The sum of squared distances of each point to its assigned center   (within-cluster sum of squares), used as a measure of cluster quality.\niterations::Int   The number of iterations of the k-means update loop that were performed.\nconverged::Bool   Indicates whether the algorithm stopped because it met the convergence   criterion (true) or because it hit the maximum number of iterations (false).\ninit_method::Symbol   The initialization method used for the run, e.g. :random or :kmeanspp.\n\n\n\n\n\n","category":"type"}]
}
