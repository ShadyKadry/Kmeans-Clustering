var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Geting-Started-with-KMeansClustering.jl","page":"Getting Started","title":"Geting Started with KMeansClustering.jl","text":"This is a short tutorial on how to use the KMeansClustering Library and visualize the results.","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"There are two options:\n\nInstallation using a local directory\n\n    pkg> add <path to library/Kmeans-Clustering>\n\nInstallation using git\n\n    pkg> add https://github.com/ShadyKadry/Kmeans-Clustering","category":"section"},{"location":"getting_started/#Basic-API","page":"Getting Started","title":"Basic API","text":"Importing is as easy as with any other package:\n\n    using KMeansClustering\n\n    # Used to select a predictable Random Numbers generator\n    using Random\n\n    # For Plotting\n    using Plots\n\nOnly one function is exposed:\n\n    my_rng = MersenneTwister(1234) # Number Generator with fixed seed\n\n    X = rand(my_rng, 2, 200) # Create an artificial dataset\n    cluster_count = 4 # Number of clusters to separate the dataset into\n\n    clustering_result = KMeansClustering.kmeans(\n        # Points, column-wise: rows are the features, cols are the points\n        X,\n\n        cluster_count,\n        # Select the KMeans-method to use\n        method=:kmeans,\n        # Select, how the initial centroids should be chosen\n        init=:random,\n        # Maximum number of iterations before the algorithm is aborted\n        maxiter=50,\n        # Tolerance of improvement between each iteration\n        tol=1e-4,\n        # Random Number Generator to use\n        rng=my_rng\n    )\n\n    @info \"Required Iterations: $(clustering_result.iterations)\"\n    @info \"Converged: $(clustering_result.converged)\"\n\nThe return value contains the result:\n\nclustering_result.centers: Matrix of center points\nclustering_result.assignments: Vector, that maps each original point to one of the centers in clustering_result.centers\nclustering_result.iterations: Number of iterations that was required\nclustering_result.converged: If true, the algorithm finished successfully (tol was reached). If false, maxiter was reached and the algorithm aborted","category":"section"},{"location":"getting_started/#Plotting","page":"Getting Started","title":"Plotting","text":"The result of the kmeans() function can be directly plotted using Plots.jl:\n\nscatter(\n    X[1, :], \n    X[2, :], \n    group=clustering_result.assignments, \n    legend=false\n)\n\nThe centers can be additionally marked:\n\nscatter!(\n    clustering_result.centers[1, :], \n    clustering_result.centers[2, :],\n    markersize=8,\n    marker=:star,\n    color=:black\n)","category":"section"},{"location":"getting_started/#Full-Script","page":"Getting Started","title":"Full Script","text":"    using KMeansClustering\n    using Random\n    using Plots\n\n    my_rng = MersenneTwister(1234) # Number Generator with fixed seed\n\n    X = rand(my_rng, 2, 200) # Create an artificial dataset\n    cluster_count = 4 # Number of clusters to separate the dataset into\n\n    clustering_result = KMeansClustering.kmeans(\n        X,                  # Points, column-wise: rows are the features, cols are the points\n        cluster_count,\n        method=:kmedoids,   # Select the KMeans-method to use\n        init=:random,\n        maxiter=50,\n        tol=1e-4,           # Tolerance of improvement between each iteration.\n        rng=my_rng          # Random Number Generator to use\n    )\n\n    @info \"Required Iterations: $(clustering_result.iterations)\"\n    @info \"Converged: $(clustering_result.converged)\"\n\n    scatter(\n        X[1, :], \n        X[2, :], \n        group=clustering_result.assignments, \n        legend=false\n    )\n\n    scatter!(\n        clustering_result.centers[1, :], \n        clustering_result.centers[2, :],\n        markersize=8,\n        marker=:star,\n        color=:black\n    )","category":"section"},{"location":"algorithms/kmedoids/#K-Medoids-Clustering","page":"K-Medoids","title":"K-Medoids Clustering","text":"","category":"section"},{"location":"algorithms/kmedoids/#Overview","page":"K-Medoids","title":"Overview","text":"The K-Medoids algorithm is a robust variant of the K-Means algorithm, that, instead of creating artificial cluster centers, uses actual data points as centers. Just as calculating the median is more robust to outliers as the average is, the K-Medoids algorithm is more resistant to noise and outliers than K-Means. This implementation uses the Partitioning Around Medoids (PAM) approach. See the references below for an explanation.\n\nComparison to K-Means:\n\nAdvantages:\nMedoids are actual data points, making results directly interpretable\nMore robust to outliers and noise compared to K-Means\nWorks with any distance metric (not limited to Euclidean distance)\nDisadvantages:\nComputationally more expensive than K-Means (O(k(n-k)^2) > O(n cdot k cdot d cdot i)) per iteration)\nwhere:\nn\n: number of data points\nk\n: number of clusters\nd\n: dimensionality\ni\n: number of iterations until convergence","category":"section"},{"location":"algorithms/kmedoids/#Implementation-Details","page":"K-Medoids","title":"Implementation Details","text":"This implementation is based on the Partitioning Around Medoids (PAM) approach as described by TU Dortmund: Partitioning Around Medoids (k-Medoids).","category":"section"},{"location":"algorithms/kmedoids/#Data-Format","page":"K-Medoids","title":"Data Format","text":"The algorithm expects data in column-major format:\n\nRows represent features/dimensions\nColumns represent individual data points/observations","category":"section"},{"location":"algorithms/kmedoids/#Usage","page":"K-Medoids","title":"Usage","text":"","category":"section"},{"location":"algorithms/kmedoids/#Basic-Usage-with-kmeans-Function","page":"K-Medoids","title":"Basic Usage with kmeans Function","text":"using KMeansClustering\n\n# Generate sample data\nX = rand(2, 100)  # 2 features, 100 observations\n\n# Perform K-Medoids clustering with 3 clusters\n# defaults to euclidian distance\nresult = kmeans(X, 3, method=:kmedoids)\n\nprintln(\"Cluster assignments: \", result.assignments)\nprintln(\"Medoids: \", result.centers)\nprintln(\"Total inertia: \", result.inertia)\nprintln(\"Converged: \", result.converged)","category":"section"},{"location":"algorithms/kmedoids/#Advanced-Usage-with-Settings-Object","page":"K-Medoids","title":"Advanced Usage with Settings Object","text":"For more control over the algorithm, use the KMedoidsAlgorithm settings object:\n\nusing KMeansClustering\nusing Random\n\nX = rand(2, 100) # Again 2 x 100\n\nsettings = KMeansClustering.KMedoidsAlgorithm(\n    X,                       # Data matrix\n    3;                       # Number of clusters\n    max_iter=100,            # Maximum iterations\n    tol=1e-4,                # Convergence tolerance\n    rng=MersenneTwister(42), # Random number generator\n    distance_fun=(a, b) -> sum((a .- b).^2)  # Distance function\n    # Alternative distance function examples:\n    # distance_fun = (a, b) -> sum(abs.(a .- b))\n    # distance_fun = (a, b) -> 1 - (dot(a, b) / (norm(a) * norm(b)))\n)\n\n# Run clustering using multiple dispatch\nresult = kmeans(settings)\n\nprintln(\"Cluster assignments: \", result.assignments)\nprintln(\"Medoids: \", result.centers)\nprintln(\"Total inertia: \", result.inertia)\nprintln(\"Converged: \", result.converged)","category":"section"},{"location":"algorithms/kmedoids/#Parameters","page":"K-Medoids","title":"Parameters","text":"For the non-overloaded version, see the main documentation page.","category":"section"},{"location":"algorithms/kmedoids/#Examples","page":"K-Medoids","title":"Examples","text":"","category":"section"},{"location":"algorithms/kmedoids/#Example-1:-Basic-Clustering","page":"K-Medoids","title":"Example 1: Basic Clustering","text":"using KMeansClustering\n\n# Create sample data: 3 Gaussian clusters\ndata = hcat(\n    randn(2, 30) .+ [0.0, 0.0],\n    randn(2, 30) .+ [5.0, 5.0],\n    randn(2, 30) .+ [10.0, 0.0]\n)\n\n# Cluster the data\nresult = kmeans(data, 3, method=:kmedoids, maxiter=100, tol=1e-4)\n\nprintln(\"Number of iterations: \", result.iterations)\nprintln(\"Converged: \", result.converged)\nprintln(\"Inertia: \", result.inertia)","category":"section"},{"location":"algorithms/kmedoids/#Example-3:-Custom-Distance-Metric","page":"K-Medoids","title":"Example 3: Custom Distance Metric","text":"using KMeansClustering\nusing LinearAlgebra\n\nX = rand(3, 100)\n\n# Use cosine distance\ncosine_distance(a, b) = begin\n    dot_product = dot(a, b)\n    norm_a = norm(a)\n    norm_b = norm(b)\n    return 1.0 - (dot_product / (norm_a * norm_b))\nend\n\nsettings = KMedoidsAlgorithm(\n    X,\n    5;\n    distance_fun=cosine_distance\n)\n\nresult = kmeans(settings)\n\nprintln(\"Number of iterations: \", result.iterations)\nprintln(\"Converged: \", result.converged)\nprintln(\"Inertia: \", result.inertia)","category":"section"},{"location":"algorithms/kmedoids/#References","page":"K-Medoids","title":"References","text":"TU Dortmund: Partitioning Around Medoids (k-Medoids)\nTU Dortmund: k-means Clustering","category":"section"},{"location":"algorithms/kmedoids/#AI-Note","page":"K-Medoids","title":"AI Note","text":"Parts of the text were compiled using generative AI, notably:\n\nA base text was produced for the Overview and then modified","category":"section"},{"location":"algorithms/kmedoids/#KMeansClustering.kmeans-Tuple{KMedoidsAlgorithm}","page":"K-Medoids","title":"KMeansClustering.kmeans","text":"kmeans(KMedoidsAlgorithm)\n\nEntry point for K-Medoids clustering using a settings object.\n\nArguments\n\nKMedoidsAlgorithm: Settings object. See object description for more information\n\nReturns\n\nA KMeansResult containing the clustering results.\n\nExample\n\nsettings = KMeansClustering.KMedoidsAlgorithm(\n    X,                  # Points, column-wise: rows are the features, cols are the points\n    cluster_count;\n    init_method=:random,\n    max_iter=50,\n)\nresult = KMeansClustering.kmeans(settings)\n\nSee also: kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=GLOBAL_RNG)\n\n\n\n\n\n","category":"method"},{"location":"algorithms/kmedoids/#KMeansClustering.KMedoidsAlgorithm","page":"K-Medoids","title":"KMeansClustering.KMedoidsAlgorithm","text":"KMedoidsAlgorithm(\n    data::AbstractMatrix{<:Real},\n    n_clusters::Integer;\n    max_iter::Integer = 100,\n    tol::Real = 10e-4,\n    rng::AbstractRNG = GLOBAL_RNG,\n    distance_fun::Function = (a::AbstractVector, b::AbstractVector) -> sum((a .- b).^2)\n)\n\nSettings specific to the KMedoids algorithm. Use this in conjunction with kmeans(KMedoidsAlgorithm) to perform K-Medoids clustering.\n\nFields:\n\ndata: Data matrix with features in rows and observations in columns\nn_clusters: Number of clusters that the dataset should be split up into\nmax_iter: Maximum number of iterations to run before aborting\ntol: Tolerance for abortion. If the improvement between iterations is smaller than tol, the algorithm aborts\nrng: Random Number Generator to use for generating the initial medoid centers\ndistance_fun: Cost function to calculate the distance between two points. This function must take two pairs of coordinates and return a number\n\n\n\n\n\n","category":"type"},{"location":"algorithms/simplekmeans/#Simple-KMeans-Clustering","page":"Simple KMeans","title":"Simple KMeans Clustering","text":"","category":"section"},{"location":"algorithms/simplekmeans/#Overview","page":"Simple KMeans","title":"Overview","text":"The simple K-Means algorithm performs clustering on a dataset following Lloyd's algorithm. Starting with k initial centers, in each iteration step every data point gets assigned to a cluster based on the nearest given center and the centroid gets updated by calculating the mean of each cluster. ","category":"section"},{"location":"algorithms/simplekmeans/#Implementation-Details","page":"Simple KMeans","title":"Implementation Details","text":"","category":"section"},{"location":"algorithms/simplekmeans/#Data-Format","page":"Simple KMeans","title":"Data Format","text":"The algorithm expects data in column-major format:\n\nRows represent features/dimensions\nColumns represent individual data points/observations","category":"section"},{"location":"algorithms/simplekmeans/#Initialization","page":"Simple KMeans","title":"Initialization","text":"The algorithm supports two kinds of initialization:\n\n:random chooses k random points from the dataset as initial centers, also called Forgy method\n:kmeanspp selects k initial centroids using the k-means++ heuristic","category":"section"},{"location":"algorithms/simplekmeans/#Usage","page":"Simple KMeans","title":"Usage","text":"","category":"section"},{"location":"algorithms/simplekmeans/#Basic-Usage-with-kmeans-Function","page":"Simple KMeans","title":"Basic Usage with kmeans Function","text":"using KMeansClustering\n\n# Generate sample data\nX = rand(2, 100)  # 2 features, 100 observations\n\n# Perform simple K-Means clustering with 3 clusters\n# defaults to :random initialization\nresult = kmeans(X, 3)\n\nprintln(\"Cluster assignments: \", result.assignments)\nprintln(\"Final centroids: \", result.centers)\nprintln(\"Total inertia: \", result.inertia)\nprintln(\"Converged: \", result.converged)","category":"section"},{"location":"algorithms/simplekmeans/#Advanced-Usage-with-Settings-Object","page":"Simple KMeans","title":"Advanced Usage with Settings Object","text":"For more control over the algorithm, use the SimpleKMeansAlgorithm settings object:\n\nusing KMeansClustering\n\nX = rand(2, 100) \n\nsettings = KMeansClustering.SimpleKMeansAlgorithm(X, 3)\n\n# Run clustering using multiple dispatch\nresult = kmeans(settings)","category":"section"},{"location":"algorithms/simplekmeans/#Parameters","page":"Simple KMeans","title":"Parameters","text":"For the non-overloaded version, see the main documentation page.","category":"section"},{"location":"algorithms/simplekmeans/#KMeansClustering.kmeans-Tuple{SimpleKMeansAlgorithm}","page":"Simple KMeans","title":"KMeansClustering.kmeans","text":"kmeans(settings::SimpleKMeansAlgorithm)\n\nEntry point for simple kmeans clustering using a settings object.\n\nArguments\n\nsettings::SimpleKMeansAlgorithm: Settings object. See object description for more information\n\nReturns\n\nA KMeansResult.\n\nExample\n\nsettings = KMeansClustering.SimpleKMeansAlgorithm(X, cluster_count)\nresult = kmeans(settings)\n\nSee also: kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=GLOBAL_RNG)\n\n\n\n\n\n","category":"method"},{"location":"algorithms/simplekmeans/#KMeansClustering.SimpleKMeansAlgorithm","page":"Simple KMeans","title":"KMeansClustering.SimpleKMeansAlgorithm","text":"SimpleKMeansAlgorithm\n\nSettings specific to the simple kmeans algorithm\n\nFields:\n\ndata: Data matrix with features in rows and observations in columns\nn_clusters: Number of clusters that the dataset should be split up into\ninit_method: Method to initialize the starting centroids\nmax_iter: Maximum number of iterations\ntol: Tolerance for abortion. If the improvement between iterations is smaller than tol, the algorithm aborts\nrng: Random Number Generator for the initial centroids\n\n\n\n\n\n","category":"type"},{"location":"#KMeansClustering","page":"Home","title":"KMeansClustering","text":"Documentation for KMeansClustering.\n\nThe module exports a single function KMeansClustering.kmeans. There are two ways to use this function. Either by passing all settings to the KMeansClustering.kmeans function directly or by creating a settings struct specific to the algorithm and passing it to the KMeansClustering.kmeans function using multiple dispatch. The latter option allows for more detailed settings. More information about the available settings can be found in the specific algorithm desciptions.\n\nA simple step-by-step description can be found in the Getting Started Guide. More usage example can be found in the examples repo folder.","category":"section"},{"location":"#KMeansClustering.KMeansClustering","page":"Home","title":"KMeansClustering.KMeansClustering","text":"KMeansClustering\n\nA Julia package for clustering algorithms, including K-Means, K-Medoids, K-Means++, BKmeans, and CKmeans.\n\nExported Functions\n\nkmeans: Perform K-Means clustering.\n\nUsage\n\njulia> using KMeansClustering\n\n\n\n\n\n","category":"module"},{"location":"#KMeansClustering.kmeans","page":"Home","title":"KMeansClustering.kmeans","text":"kmeans(settings::SimpleKMeansAlgorithm)\n\nEntry point for simple kmeans clustering using a settings object.\n\nArguments\n\nsettings::SimpleKMeansAlgorithm: Settings object. See object description for more information\n\nReturns\n\nA KMeansResult.\n\nExample\n\nsettings = KMeansClustering.SimpleKMeansAlgorithm(X, cluster_count)\nresult = kmeans(settings)\n\nSee also: kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=GLOBAL_RNG)\n\n\n\n\n\nkmeans(KMedoidsAlgorithm)\n\nEntry point for K-Medoids clustering using a settings object.\n\nArguments\n\nKMedoidsAlgorithm: Settings object. See object description for more information\n\nReturns\n\nA KMeansResult containing the clustering results.\n\nExample\n\nsettings = KMeansClustering.KMedoidsAlgorithm(\n    X,                  # Points, column-wise: rows are the features, cols are the points\n    cluster_count;\n    init_method=:random,\n    max_iter=50,\n)\nresult = KMeansClustering.kmeans(settings)\n\nSee also: kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=GLOBAL_RNG)\n\n\n\n\n\nkmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=Random.GLOBAL_RNG)\n\nHigh-level entry point for k-means clustering.\n\nArguments\n\nX: data matrix with features in rows and observations in columns.\nk: number of clusters.\n\nKeyword arguments\n\nmethod: algorithm selector, see below (:kmedoids)\ninit: initialization strategy (:random, :kmeanspp).\nmaxiter: maximum number of Lloyd iterations.\ntol: tolerance for convergence.\nrng: random number generator.\n\nReturns a KMeansResult.\n\nAvailable algorithms:\n\nK-Medoids (method=:kmedoids):   As described by TU Dortmund: Partitioning Around Medoids (k-Medoids)   Unlike typical K-Means, K-Medoids chooses its cluster centers from the given points X instead of calculating   artificial ones.\n\n\n\n\n\n","category":"function"},{"location":"#KMeansClustering.KMeansResult","page":"Home","title":"KMeansClustering.KMeansResult","text":"KMeansResult\n\nThis type stores the outcome of a k-means clustering run.\n\nConventions:\n\nThe data matrix X is assumed to have observations in columns and features in rows:   size(X, 1) = number of features   size(X, 2) = number of points\nThe centers matrix follows the same convention: each column is a cluster center.\n\nFields:\n\ncenters::Matrix{T}   The final cluster centers as a d√ók matrix, where d is the number of features   and k is the number of clusters.\nassignments::Vector{Int}   Cluster assignment for each data point as a vector of length n, where   n is the number of points (columns of X). The i-th entry is an integer   in 1:k indicating the cluster index of point i.\ninertia::T   The sum of squared distances of each point to its assigned center   (within-cluster sum of squares), used as a measure of cluster quality.\niterations::Int   The number of iterations of the k-means update loop that were performed.\nconverged::Bool   Indicates whether the algorithm stopped because it met the convergence   criterion (true) or because it hit the maximum number of iterations (false).\ninit_method::Symbol   The initialization method used for the run, e.g. :random or :kmeanspp.\n\n\n\n\n\n","category":"type"}]
}
