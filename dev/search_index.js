var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Geting-Started-with-KMeansClustering.jl","page":"Getting Started","title":"Geting Started with KMeansClustering.jl","text":"This is a short tutorial on how to use the KMeansClustering Library and visualize the results.","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"There are two options:\n\nInstallation using a local directory\n\n    pkg> add <path to library/Kmeans-Clustering>\n\nInstallation using git\n\n    pkg> add https://github.com/ShadyKadry/Kmeans-Clustering","category":"section"},{"location":"getting_started/#Basic-API","page":"Getting Started","title":"Basic API","text":"Importing is as easy as with any other package:\n\n    using KMeansClustering\n\n    # Used to select a predictable Random Numbers generator\n    using Random\n\n    # For Plotting\n    using Plots\n\nOnly one function is exposed:\n\n    my_rng = MersenneTwister(1234) # Number Generator with fixed seed\n\n    X = rand(my_rng, 2, 200) # Create an artificial dataset\n    cluster_count = 4 # Number of clusters to separate the dataset into\n\n    clustering_result = KMeansClustering.kmeans(\n        # Points, column-wise: rows are the features, cols are the points\n        X,\n\n        cluster_count,\n        # Select the KMeans-method to use\n        method=:kmeans,\n        # Select, how the initial centroids should be chosen\n        init=:random,\n        # Maximum number of iterations before the algorithm is aborted\n        maxiter=50,\n        # Tolerance of improvement between each iteration\n        tol=1e-4,\n        # Random Number Generator to use\n        rng=my_rng\n    )\n\n    @info \"Required Iterations: $(clustering_result.iterations)\"\n    @info \"Converged: $(clustering_result.converged)\"\n\nThe return value contains the result:\n\nclustering_result.centers: Matrix of center points\nclustering_result.assignments: Vector, that maps each original point to one of the centers in clustering_result.centers\nclustering_result.iterations: Number of iterations that was required\nclustering_result.converged: If true, the algorithm finished successfully (tol was reached). If false, maxiter was reached and the algorithm aborted","category":"section"},{"location":"getting_started/#Plotting","page":"Getting Started","title":"Plotting","text":"The result of the kmeans() function can be directly plotted using Plots.jl:\n\nscatter(\n    X[1, :], \n    X[2, :], \n    group=clustering_result.assignments, \n    legend=false\n)\n\nThe centers can be additionally marked:\n\nscatter!(\n    clustering_result.centers[1, :], \n    clustering_result.centers[2, :],\n    markersize=8,\n    marker=:star,\n    color=:black\n)","category":"section"},{"location":"getting_started/#Full-Script","page":"Getting Started","title":"Full Script","text":"    using KMeansClustering\n    using Random\n    using Plots\n\n    my_rng = MersenneTwister(1234) # Number Generator with fixed seed\n\n    X = rand(my_rng, 2, 200) # Create an artificial dataset\n    cluster_count = 4 # Number of clusters to separate the dataset into\n\n    clustering_result = KMeansClustering.kmeans(\n        X,                  # Points, column-wise: rows are the features, cols are the points\n        cluster_count,\n        method=:kmedoids,   # Select the KMeans-method to use\n        init=:random,\n        maxiter=50,\n        tol=1e-4,           # Tolerance of improvement between each iteration.\n        rng=my_rng          # Random Number Generator to use\n    )\n\n    @info \"Required Iterations: $(clustering_result.iterations)\"\n    @info \"Converged: $(clustering_result.converged)\"\n\n    scatter(\n        X[1, :], \n        X[2, :], \n        group=clustering_result.assignments, \n        legend=false\n    )\n\n    scatter!(\n        clustering_result.centers[1, :], \n        clustering_result.centers[2, :],\n        markersize=8,\n        marker=:star,\n        color=:black\n    )","category":"section"},{"location":"algorithms/kmedoids/#K-Medoids-Clustering","page":"KMedoids","title":"K-Medoids Clustering","text":"","category":"section"},{"location":"algorithms/kmedoids/#Overview","page":"KMedoids","title":"Overview","text":"The K-Medoids algorithm is a robust variant of the K-Means algorithm, that, instead of creating artificial cluster centers, uses actual data points as centers. Just as calculating the median is more robust to outliers as the average is, the K-Medoids algorithm is more resistant to noise and outliers than K-Means.\n\nComparison to K-Means:\n\nAdvantages:\nMedoids are actual data points, making results directly interpretable\nMore robust to outliers and noise compared to K-Means\nWorks with any distance metric (not limited to Euclidean distance)\nDisadvantages:\nComputationally more expensive than K-Means (O(k(n-k)^2)) per iteration)","category":"section"},{"location":"algorithms/kmedoids/#Implementation-Details","page":"KMedoids","title":"Implementation Details","text":"This implementation is based on the Partitioning Around Medoids (PAM) approach as described by E.M. Mirkes, University of Leicester, 2011.","category":"section"},{"location":"algorithms/kmedoids/#Data-Format","page":"KMedoids","title":"Data Format","text":"The algorithm expects data in column-major format:\n\nRows represent features/dimensions\nColumns represent individual data points/observations","category":"section"},{"location":"algorithms/kmedoids/#Usage","page":"KMedoids","title":"Usage","text":"","category":"section"},{"location":"algorithms/kmedoids/#Basic-Usage-with-kmeans-Function","page":"KMedoids","title":"Basic Usage with kmeans Function","text":"using KMeansClustering\n\n# Generate sample data\nX = rand(2, 100)  # 2 features, 100 observations\n\n# Perform K-Medoids clustering with 3 clusters\n# defaults to euclidian distance\nresult = kmeans(X, 3, method=:kmedoids)\n\nprintln(\"Cluster assignments: \", result.assignments)\nprintln(\"Medoids: \", result.centers)\nprintln(\"Total inertia: \", result.inertia)\nprintln(\"Converged: \", result.converged)","category":"section"},{"location":"algorithms/kmedoids/#Advanced-Usage-with-Settings-Object","page":"KMedoids","title":"Advanced Usage with Settings Object","text":"For more control over the algorithm, use the KMedoidsAlgorithm settings object:\n\nusing KMeansClustering\nusing Random\n\nX = rand(2, 100) # Again 2 x 100\n\nsettings = KMeansClustering.KMedoidsAlgorithm(\n    X,                       # Data matrix\n    3;                       # Number of clusters\n    max_iter=100,            # Maximum iterations\n    tol=1e-4,                # Convergence tolerance\n    rng=MersenneTwister(42), # Random number generator\n    distance_fun=(a, b) -> sum((a .- b).^2)  # Distance function\n    # Alternative distance function examples:\n    # distance_fun = (a, b) -> sum(abs.(a .- b))\n    # distance_fun = (a, b) -> 1 - (dot(a, b) / (norm(a) * norm(b)))\n)\n\n# Run clustering using multiple dispatch\nresult = kmeans(settings)","category":"section"},{"location":"algorithms/kmedoids/#Parameters","page":"KMedoids","title":"Parameters","text":"For the non-overloaded version, see the main documentation page.","category":"section"},{"location":"algorithms/kmedoids/#Examples","page":"KMedoids","title":"Examples","text":"","category":"section"},{"location":"algorithms/kmedoids/#Example-1:-Basic-Clustering","page":"KMedoids","title":"Example 1: Basic Clustering","text":"using KMeansClustering\n\n# Create sample data: 3 Gaussian clusters\ndata = hcat(\n    randn(2, 30) .+ [0.0, 0.0],\n    randn(2, 30) .+ [5.0, 5.0],\n    randn(2, 30) .+ [10.0, 0.0]\n)\n\n# Cluster the data\nresult = kmeans(data, 3, method=:kmedoids, maxiter=100, tol=1e-4)\n\nprintln(\"Number of iterations: \", result.iterations)\nprintln(\"Converged: \", result.converged)\nprintln(\"Inertia: \", result.inertia)","category":"section"},{"location":"algorithms/kmedoids/#Example-3:-Custom-Distance-Metric","page":"KMedoids","title":"Example 3: Custom Distance Metric","text":"using KMeansClustering\nusing LinearAlgebra\n\n# Data with angular relationships\nX = rand(3, 100)\n\n# Use cosine distance\ncosine_distance(a, b) = begin\n    dot_product = dot(a, b)\n    norm_a = norm(a)\n    norm_b = norm(b)\n    return 1.0 - (dot_product / (norm_a * norm_b))\nend\n\nsettings = KMedoidsAlgorithm(\n    X,\n    5;\n    distance_fun=cosine_distance\n)\n\nresult = kmeans(settings)","category":"section"},{"location":"algorithms/kmedoids/#References","page":"KMedoids","title":"References","text":"E.M. Mirkes, \"K-means and K-medoids applet\", University of Leicester, 2011. http://leicestermath.org.uk/KmeansKmedoids/Kmeans_Kmedoids.html","category":"section"},{"location":"algorithms/kmedoids/#KMeansClustering.kmeans-Tuple{KMedoidsAlgorithm}","page":"KMedoids","title":"KMeansClustering.kmeans","text":"kmeans(KMedoidsAlgorithm)\n\nEntry point for K-Medoids clustering using a settings object instead.\n\nArguments\n\nKMedoidsAlgorithm: Settings object. See object description for more information\n\nReturns\n\nA KMeansResult containing the clustering results.\n\nExample\n\nsettings = KMeansClustering.KMedoidsAlgorithm(\n    X,                  # Points, column-wise: rows are the features, cols are the points\n    cluster_count;\n    init_method=:random,\n    max_iter=50,\n)\nresult = KMeansClustering.kmeans(settings)\n\nSee also: kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=GLOBAL_RNG)\n\n\n\n\n\n","category":"method"},{"location":"algorithms/kmedoids/#KMeansClustering.KMedoidsAlgorithm","page":"KMedoids","title":"KMeansClustering.KMedoidsAlgorithm","text":"KMedoidsAlgorithm\n\nSettings specific to the KMedoids algorithm\n\nFields:\n- `data`: Data matrix with features in rows and observations in columns\n- `n_clusters`: Number of clusters that the dataset should be split up into\n- `max_iter`: Maximum number of iterations to run before aborting\n- `tol`: Tolerance for abortion. If the improvement between iterations is smaller than `tol`, the algorithm aborts\n- `rng`: Random Number Generator to use for generating the initial medoid centers\n- `distance_fun`: Cost function to calculate the distance between two points. This function must take two pairs of coordinates and return a number\n\n\n\n\n\n","category":"type"},{"location":"#KMeansClustering","page":"Home","title":"KMeansClustering","text":"Documentation for KMeansClustering.\n\nThe module exports a single function KMeansClustering.kmeans. There are two ways to use this function. Either by passing all settings to the KMeansClustering.kmeans function directly or by creating a sttings struct specific to the algorithm and passing it to the KMeansClustering.kmeans function using multiple dispatch. The latter option allows for more detailed settings. More information about the available settings can be found in the specific algorithm desciptions.\n\nA simple step-by-step description can be found in the Getting Started Guide. More usage example can be found in the examples repo folder.","category":"section"},{"location":"#KMeansClustering.KMeansClustering","page":"Home","title":"KMeansClustering.KMeansClustering","text":"KMeansClustering\n\nA Julia package for clustering algorithms, including K-Means, K-Medoids, K-Means++, BKmeans, and CKmeans.\n\nExported Functions\n\nkmeans: Perform K-Means clustering.\n\nUsage\n\njulia> using KMeansClustering\n\n\n\n\n\n","category":"module"},{"location":"#KMeansClustering.kmeans","page":"Home","title":"KMeansClustering.kmeans","text":"kmeans(KMedoidsAlgorithm)\n\nEntry point for K-Medoids clustering using a settings object instead.\n\nArguments\n\nKMedoidsAlgorithm: Settings object. See object description for more information\n\nReturns\n\nA KMeansResult containing the clustering results.\n\nExample\n\nsettings = KMeansClustering.KMedoidsAlgorithm(\n    X,                  # Points, column-wise: rows are the features, cols are the points\n    cluster_count;\n    init_method=:random,\n    max_iter=50,\n)\nresult = KMeansClustering.kmeans(settings)\n\nSee also: kmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=GLOBAL_RNG)\n\n\n\n\n\nkmeans(X, k; method=:kmeans, init=:random, maxiter=100, tol=1e-4, rng=Random.GLOBAL_RNG)\n\nHigh-level entry point for k-means clustering.\n\nArguments\n\nX: data matrix with features in rows and observations in columns.\nk: number of clusters.\n\nKeyword arguments\n\nmethod: algorithm selector, see below (:kmedoids)\ninit: initialization strategy (:random, :kmeanspp).\nmaxiter: maximum number of Lloyd iterations.\ntol: tolerance for convergence.\nrng: random number generator.\n\nReturns a KMeansResult.\n\nAvailable algorithms:\n\nK-Medoids (method=:kmedoids):   As described by E.M. Mirkes, K-means and K-medoids applet. University of Leicester, 2011   Unlike typical K-Means, K-Medoids chooses its cluster centers from the given points X instead of calculating   artificial ones.\n\n\n\n\n\n","category":"function"},{"location":"#KMeansClustering.KMeansResult","page":"Home","title":"KMeansClustering.KMeansResult","text":"KMeansResult\n\nThis type stores the outcome of a k-means clustering run.\n\nConventions:\n\nThe data matrix X is assumed to have observations in columns and features in rows:   size(X, 1) = number of features   size(X, 2) = number of points\nThe centers matrix follows the same convention: each column is a cluster center.\n\nFields:\n\ncenters::Matrix{T}   The final cluster centers as a d√ók matrix, where d is the number of features   and k is the number of clusters.\nassignments::Vector{Int}   Cluster assignment for each data point as a vector of length n, where   n is the number of points (columns of X). The i-th entry is an integer   in 1:k indicating the cluster index of point i.\ninertia::T   The sum of squared distances of each point to its assigned center   (within-cluster sum of squares), used as a measure of cluster quality.\niterations::Int   The number of iterations of the k-means update loop that were performed.\nconverged::Bool   Indicates whether the algorithm stopped because it met the convergence   criterion (true) or because it hit the maximum number of iterations (false).\ninit_method::Symbol   The initialization method used for the run, e.g. :random or :kmeanspp.\n\n\n\n\n\n","category":"type"}]
}
